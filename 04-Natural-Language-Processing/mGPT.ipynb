{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# mGPT - Multilingual GPT model\n",
        "\n",
        "[Huggingface model card](https://huggingface.co/ai-forever/mGPT)\n",
        "\n",
        "Supported languages:\n",
        "Arabic, Hebrew, Vietnamese, Indonesian, Javanese, Malay, Tagalog, Latvian, Lithuanian, Basque, Malayalam, Tamil, Telugu, Armenian, Bengali, Marathi, Hindi, Urdu, Afrikaans, Danish, English, German, Swedish, French, Italian, Portuguese, Romanian, Spanish, Greek, Ossetian, Tajik, Persian, Japanese, Georgian, Korean, Thai, Buryat, Kalmyk, Mongolian, Swahili, Yoruba, Belarusian, Bulgarian, Russian, Ukrainian, Polish, Burmese, Uzbek, Bashkir, Kazakh, Kyrgyz, Tatar, Azerbaijani, Chuvash, Turkish, Turkmen, Tuvan, Yakut, Estonian, Finnish, Hungarian"
      ],
      "metadata": {
        "id": "KA1E5qL9A_ng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install transformers\n",
        "!pip install transformers[torch]\n",
        "!pip install accelerate -U"
      ],
      "metadata": {
        "cellView": "form",
        "id": "lvr1bVAp7vxW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d168c39-32b8-45ec-e62b-7f418e4843d3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers[torch]\n",
            "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers[torch])\n",
            "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers[torch])\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m82.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers[torch])\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.65.0)\n",
            "Requirement already satisfied: torch!=1.12.0,>=1.9 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.0.1+cu118)\n",
            "Collecting accelerate>=0.20.2 (from transformers[torch])\n",
            "  Downloading accelerate-0.20.3-py3-none-any.whl (227 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.6/227.6 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.20.2->transformers[torch]) (5.9.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers[torch]) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers[torch]) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch!=1.12.0,>=1.9->transformers[torch]) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch!=1.12.0,>=1.9->transformers[torch]) (16.0.5)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=1.12.0,>=1.9->transformers[torch]) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.12.0,>=1.9->transformers[torch]) (1.3.0)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers, accelerate\n",
            "Successfully installed accelerate-0.20.3 huggingface-hub-0.15.1 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.30.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->accelerate) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->accelerate) (16.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->accelerate) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->accelerate) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load libraries\n",
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "from transformers import TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "LF80bX_KiY8z"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "aFSFzFVp7hEw"
      },
      "outputs": [],
      "source": [
        "#@title Get Model\n",
        "\n",
        "model = \"ai-forever/mGPT\"\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model)\n",
        "model = GPT2LMHeadModel.from_pretrained(model, pad_token_id=tokenizer.eos_token_id)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate"
      ],
      "metadata": {
        "id": "5npiDHh4pLRO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Generate\n",
        "#@markdown Text to start with\n",
        "prompt = \"\" #@param {type: \"string\"}\n",
        "#@markdown Length of generated text in tokens (100 tokens is about 75 words)\n",
        "max_length = 250 #@param {type: \"integer\"}\n",
        "#@markdown Temperature. Best results in range 0.8-2\n",
        "temperature = 0.8  #@param {type:\"slider\", min:0, max:2, step:0.1}\n",
        "inputs = tokenizer(prompt, add_special_tokens=False, return_tensors=\"pt\")[\"input_ids\"]\n",
        "prompt_length = len(tokenizer.decode(inputs[0]))\n",
        "outputs = model.generate(inputs, max_length=max_length, do_sample=True, temperature=temperature)\n",
        "generated = prompt + tokenizer.decode(outputs[0])[prompt_length + 1 :]\n",
        "\n",
        "print(generated)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "8wQDY_GE8F2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finetuning\n",
        "\n",
        "**Doesn't work on Colab free plan :(**\n",
        "\n",
        "You can fintune the model on your own texts.\n",
        "Collect the dataset of texts and save them to the same `.txt` file. Texts should be separated with `<|endoftext|>`\n",
        "Check [this example](https://raw.githubusercontent.com/ai-forever/mgpt/main/data/sah.txt) in Sakha language.\n",
        "Upload the file to Colab notebook and paste the path to the `dataset` param in the following cell\n"
      ],
      "metadata": {
        "id": "e32IwUzlg9__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Mount Google Drive\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "rYUTCofZphcJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load dataset\n",
        "dataset_path = \"/content/sah.txt\" #@param {\"type\": \"string\"}\n",
        "\n",
        "train_dataset = TextDataset(tokenizer=tokenizer,file_path=dataset_path,block_size=64)\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Nj1wb_cvkkp6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Set Params\n",
        "#@markdown The output directory where model will be saved (you can store it on the drive to reuse it later)\n",
        "model_dir = \"/content/drive/MyDrive/AI/mGPT\" #@param {\"type\": \"string\"}\n",
        "#@markdown Overwrite the content of the output directory\n",
        "overwrite_output_dir=True #@param {\"type\": \"boolean\"}\n",
        "#@markdown Number of training epochs\n",
        "num_train_epochs=2 #@param {\"type\": \"integer\"}\n",
        "#@markdown Batch size for training\n",
        "per_device_train_batch_size=4 #@param {\"type\": \"integer\"}\n",
        "#@markdown Batch size for evaluation\n",
        "per_device_eval_batch_size=4 #@param {\"type\": \"integer\"}\n",
        "#@markdown Number of warmup steps for learning rate scheduler\n",
        "warmup_steps=10 #@param {\"type\": \"integer\"}\n",
        "#@markdown To make \"virtual\" batch size larger\n",
        "gradient_accumulation_steps=16 #@param {\"type\": \"integer\"}\n",
        "#@markdown Learning rate (set smaller learning rate for smaller datasets)\n",
        "lr = 0.00001 #@param {type:\"slider\", min:1e-5, max:1e-4, step:4.5e-5}\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./output/\", #The output directory\n",
        "    overwrite_output_dir=overwrite_output_dir, #overwrite the content of the output directory\n",
        "    num_train_epochs=num_train_epochs, # number of training epochs\n",
        "    per_device_train_batch_size=per_device_train_batch_size, # batch size for training\n",
        "    per_device_eval_batch_size=per_device_eval_batch_size,  # batch size for evaluation\n",
        "    warmup_steps=warmup_steps,# number of warmup steps for learning rate scheduler\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps, # to make \"virtual\" batch size larger\n",
        "    )\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        "    optimizers = (torch.optim.AdamW(model.parameters(),lr=lr),None) # Optimizer and lr scheduler\n",
        ")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "7osTYv6Xk_y3"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run Finetuning\n",
        "#@markdown This will run the finetuning and save the model after that\n",
        "trainer.train()\n",
        "trainer.save_model(model_dir)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "_mhes6I3lzQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate with finetuned model"
      ],
      "metadata": {
        "id": "t4vE2ZMNpP91"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load finetuned model\n",
        "#@markdown The directory where finetuned model is stored\n",
        "model_dir = \"/content/drive/MyDrive/AI/mGPT\" #@param {\"type\": \"string\"}\n",
        "\n",
        "\n",
        "model_name_or_path = \"ai-forever/mGPT\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name_or_path)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_dir).to(DEVICE)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "W693BEaCl0nr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Generate\n",
        "#@markdown Text to start with\n",
        "prompt = \"\" #@param {type: \"string\"}\n",
        "#@markdown Length of generated text in tokens (100 tokens is about 75 words)\n",
        "max_length = 250 #@param {type: \"integer\"}\n",
        "#@markdown Temperature. Best results in range 0.8-2\n",
        "temperature = 0.8  #@param {type:\"slider\", min:0, max:2, step:0.1}\n",
        "inputs = tokenizer(prompt, add_special_tokens=False, return_tensors=\"pt\")[\"input_ids\"]\n",
        "prompt_length = len(tokenizer.decode(inputs[0]))\n",
        "outputs = model.generate(inputs, max_length=max_length, do_sample=True, temperature=temperature)\n",
        "generated = prompt + tokenizer.decode(outputs[0])[prompt_length + 1 :]\n",
        "\n",
        "print(generated)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Fow5ew3Gp4cn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}